{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained models and compute the brier score on the test set. Save for plotting.\n",
    "\n",
    "This is needed bc didn't record Brier Score on the training runs. Thus compute Brier Score and save to dict for plotting in another file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import yaml\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, remove_self_loops, add_self_loops\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling\n",
    "\n",
    "from gnn import GCNEncoder, GAE, VGAE, VariationalMLPDecoder, MLPDecoder, posterior_predictive_metrics\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"'train_test_split_edges' is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\") # supress annoying warning which is internal to pytorch-geometric loading (I think)\n",
    "\n",
    "from config import GNN_DIR, DATA_DIR\n",
    "log_dir =  GNN_DIR + \"logs/\"\n",
    "model_dir = GNN_DIR + \"models/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single model on a single dataset on a single run and compute Brier Score\n",
    "\n",
    "dataset_name = \"Cora\"\n",
    "#model = \"ks\"\n",
    "run = 0\n",
    "seed = 5 * run\n",
    "\n",
    "# Define model hyperparameters (should match your training configuration)\n",
    "latent_distrib = 'beta'  # change if using a different distribution\n",
    "pre_str = 'edge_' if latent_distrib in ['ks', 'beta', 'tanh-normal'] else ''\n",
    "\n",
    "model_path = f\"{model_dir}\" + pre_str + f\"{latent_distrib}_{dataset_name}_{run}.pt\"\n",
    "\n",
    "# Define model hyperparameters (should match your training configuration)\n",
    "hidden_channels = 32\n",
    "out_channels = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_decode(model, x, train_pos_edge_index, neg_edge_index):\n",
    "    embed = model(x, train_pos_edge_index)\n",
    "    distrib_pos = model.decode(embed, train_pos_edge_index)\n",
    "    distrib_neg = model.decode(embed, neg_edge_index)\n",
    "    return distrib_pos, distrib_neg\n",
    "\n",
    "def posterior_predictive_metrics(post_pred_samples, labels, print_metrics=False):\n",
    "    #labels = torch.cat([torch.ones(pos_edge_index.shape[1]), torch.zeros(neg_edge_index.shape[1])], dim=0)\n",
    "\n",
    "    #post_pred_samples = torch.stack(post_pred_samples) # shape (num_samples, num_edges)\n",
    "    mean, std = post_pred_samples.mean(dim=0), post_pred_samples.std(dim=0)\n",
    "    error = (mean - labels).abs()\n",
    "\n",
    "    #### metrics\n",
    "    # quality of the uncertainty\n",
    "    pearson_corr = torch.corrcoef(torch.stack((error, std)))[0, 1] # shape (1)\n",
    "\n",
    "    # quality of uncertainty over positive and negative edges\n",
    "    pos_mask, neg_mask = (labels == 1), (labels == 0)\n",
    "    #assert len(pos_mask) == len(neg_mask), \"Positive and negative masks must be the same length\"\n",
    "    num_unique_error = torch.unique(error).shape[0]\n",
    "    num_unique_std = torch.unique(std).shape[0]\n",
    "    #print(f\"\\tUnique error values: {num_unique_error}, unique std values: {num_unique_std}\")\n",
    "    \n",
    "    pearson_corr_pos = torch.corrcoef(torch.stack((std[pos_mask], error[pos_mask])))[0, 1]\n",
    "    pearson_corr_neg = torch.corrcoef(torch.stack((std[neg_mask], error[neg_mask])))[0, 1]\n",
    "\n",
    "    # brier score\n",
    "    bs = (mean - labels)**2\n",
    "    bs_pos = bs[pos_mask]\n",
    "    bs_neg = bs[neg_mask]\n",
    "\n",
    "    pred_binary = (mean >= 0.5).int()  # Threshold at 0.5 to get binary predictions\n",
    "    # quality of mean of posterior predictive as predictor.\n",
    "    # auc and ap may not make sense, as the mean is not really a probability.\n",
    "    auc, ap, f1 = roc_auc_score(labels, mean), average_precision_score(labels, mean), f1_score(labels, pred_binary)\n",
    "\n",
    "    if print_metrics:\n",
    "        print(f\"\\tMean predictor performance: AUC: {auc:.4f}, AP: {ap:.4f}, F1: {f1:.4f}\")\n",
    "        print(f\"\\tPearson correlation: {pearson_corr:.4f}, pos: {pearson_corr_pos:.4f}, neg: {pearson_corr_neg:.4f}\")\n",
    "        print(f\"\\tBrier Score: {bs.mean().float():.4f}, pos: {bs_pos.mean().float():.4f}, neg: {bs_neg.mean().float():.4f}\")\n",
    "\n",
    "    metrics = {\n",
    "        'abs_error': error.mean().float(),\n",
    "        'brier_score': bs.mean().float(),\n",
    "        'std': std.mean().float(),\n",
    "        'pearson_corr': pearson_corr.float(),\n",
    "        'pearson_corr_pos': pearson_corr_pos.float(),\n",
    "        'pearson_corr_neg': pearson_corr_neg.float(),\n",
    "        'auc': auc,\n",
    "        'ap': ap,\n",
    "        'f1': f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def test_posterior_predictive(model, x, pos_edge_index, neg_edge_index, num_samples=30, viz=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        post_pred_samples = []\n",
    "        labels = torch.cat([torch.ones(pos_edge_index.shape[1]), torch.zeros(neg_edge_index.shape[1])], dim=0)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # encode and decode\n",
    "            distrib_pos, distrib_neg = encode_and_decode(model, x, pos_edge_index, neg_edge_index)\n",
    "\n",
    "            # sample\n",
    "            edge_prob_sample_pos = distrib_pos.rsample()\n",
    "            edge_prob_sample_neg = distrib_neg.rsample()\n",
    "\n",
    "            # sample the bernoulli likelihood\n",
    "            post_pred_samples.append(torch.cat([\n",
    "                torch.bernoulli(edge_prob_sample_pos), \n",
    "                torch.bernoulli(edge_prob_sample_neg)], \n",
    "                dim=0))\n",
    "            \n",
    "    post_pred_samples = torch.stack(post_pred_samples) # shape (num_samples, num_edges)\n",
    "    metrics = posterior_predictive_metrics(post_pred_samples, labels)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "\tEdge Latent: ks on Cora\n",
      "\tBrier Score across Runs: 0.08209 +/- 0.01029\n",
      "\tEdge Latent: beta on Cora\n",
      "\tBrier Score across Runs: 0.11650 +/- 0.01380\n",
      "\tEdge Latent: tanh-normal on Cora\n",
      "\tBrier Score across Runs: 0.11521 +/- 0.04466\n",
      "Dataset: Citeseer\n",
      "\tEdge Latent: ks on Citeseer\n",
      "\tBrier Score across Runs: 0.10402 +/- 0.00802\n",
      "\tEdge Latent: beta on Citeseer\n",
      "\tBrier Score across Runs: 0.14022 +/- 0.01847\n",
      "\tEdge Latent: tanh-normal on Citeseer\n",
      "\tBrier Score across Runs: 0.11059 +/- 0.00429\n",
      "Dataset: Pubmed\n",
      "\tEdge Latent: ks on Pubmed\n",
      "\tBrier Score across Runs: 0.06075 +/- 0.00430\n",
      "\tEdge Latent: beta on Pubmed\n",
      "\tBrier Score across Runs: 0.06916 +/- 0.00249\n",
      "\tEdge Latent: tanh-normal on Pubmed\n",
      "\tBrier Score across Runs: 0.12017 +/- 0.04162\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "def compute_test_metrics(dataset_name, latent_distrib, run, seed):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #print(f\"Using device: {device}\")\n",
    "\n",
    "    dataset = Planetoid(DATA_DIR, dataset_name, transform=T.NormalizeFeatures())\n",
    "    data = dataset[0]\n",
    "    # Remove masks if any, then create a train/test split\n",
    "    data.train_mask = data.val_mask = data.test_mask = None\n",
    "    torch.manual_seed(seed)\n",
    "    data_tts = train_test_split_edges(data)\n",
    "\n",
    "    model = GAE(\n",
    "        GCNEncoder(dataset.num_features, hidden_channels, out_channels),\n",
    "        VariationalMLPDecoder(out_channels, out_channels, latent_distrib=latent_distrib)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    x = data.x.to(device)\n",
    "    train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "\n",
    "    #print(f\"Edge Latent: {latent_distrib} on {dataset_name} - Run {run}\")\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = GAE(\n",
    "        GCNEncoder(dataset.num_features, hidden_channels, out_channels),\n",
    "        VariationalMLPDecoder(out_channels, out_channels, latent_distrib=latent_distrib)\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the trained model state\n",
    "    pre_str = 'edge_' if latent_distrib in ['ks', 'beta', 'tanh-normal'] else ''\n",
    "    model_path = f\"{model_dir}\" + pre_str + f\"{latent_distrib}_{dataset_name}_{run}.pt\"\n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare test edges and features\n",
    "    x = data.x.to(device)\n",
    "    pos_edge_index = data.test_pos_edge_index.to(device)\n",
    "    neg_edge_index = data.test_neg_edge_index.to(device)\n",
    "\n",
    "    metrics = test_posterior_predictive(model, x, pos_edge_index, neg_edge_index)\n",
    "    return metrics\n",
    "#print(metrics)\n",
    "\n",
    "from collections import defaultdict\n",
    "brier_scores = defaultdict(dict)\n",
    "for dataset_name in [\"Cora\", \"Citeseer\", \"Pubmed\"]:\n",
    "    brier_scores[dataset_name] = defaultdict(list)\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    for latent_distrib in [\"ks\", \"beta\", \"tanh-normal\"]:\n",
    "        print(f\"\\tEdge Latent: {latent_distrib} on {dataset_name}\")\n",
    "        for run in [0, 1, 2, 3, 4]:\n",
    "            metrics = compute_test_metrics(dataset_name, latent_distrib, run, 5*run)\n",
    "            brier_scores[dataset_name][latent_distrib].append(metrics['brier_score'].item())\n",
    "\n",
    "        print(f\"\\tBrier Score across Runs: {np.mean(brier_scores[dataset_name][latent_distrib]):.5f} +/- {np.std(brier_scores[dataset_name][latent_distrib]):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'Cora': defaultdict(<class 'list'>, {'ks': [0.08901433646678925, 0.07873287051916122, 0.09745624661445618, 0.06736031174659729, 0.07789163291454315], 'beta': [0.10884460061788559, 0.09924519807100296, 0.13080644607543945, 0.10886358469724655, 0.1347469985485077], 'tanh-normal': [0.08107948303222656, 0.17799915373325348, 0.07496626675128937, 0.08115749061107635, 0.16086654365062714]}), 'Citeseer': defaultdict(<class 'list'>, {'ks': [0.10626861453056335, 0.11789742857217789, 0.10029914230108261, 0.10188400000333786, 0.09374602884054184], 'beta': [0.11231989413499832, 0.16957508027553558, 0.13816359639167786, 0.1460866928100586, 0.13496580719947815], 'tanh-normal': [0.11061416566371918, 0.11421610414981842, 0.11055921763181686, 0.11478753387928009, 0.10277044028043747]}), 'Pubmed': defaultdict(<class 'list'>, {'ks': [0.06285273283720016, 0.05707894638180733, 0.06074433773756027, 0.05549626424908638, 0.06759877502918243], 'beta': [0.06824583560228348, 0.06942050158977509, 0.07306458055973053, 0.06535298377275467, 0.06973250955343246], 'tanh-normal': [0.12236674129962921, 0.19141246378421783, 0.12964662909507751, 0.07344915717840195, 0.08398077636957169]})})\n"
     ]
    }
   ],
   "source": [
    "print(brier_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Cora', 'Citeseer', 'Pubmed'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run point estimator GNN\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "def compute_test_metrics_base(dataset_name, run, seed):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #print(f\"Using device: {device}\")\n",
    "\n",
    "    dataset = Planetoid(DATA_DIR, dataset_name, transform=T.NormalizeFeatures())\n",
    "    data = dataset[0]\n",
    "    # Remove masks if any, then create a train/test split\n",
    "    data.train_mask = data.val_mask = data.test_mask = None\n",
    "    torch.manual_seed(seed)\n",
    "    data_tts = train_test_split_edges(data)\n",
    "\n",
    "    model = GAE(GCNEncoder(dataset.num_features, hidden_channels, out_channels),\n",
    "                MLPDecoder(out_channels, out_channels)\n",
    "                )\n",
    "    model = model.to(device)\n",
    "    x = data.x.to(device)\n",
    "    train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "\n",
    "    #print(f\"Edge Latent: {latent_distrib} on {dataset_name} - Run {run}\")\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = GAE(\n",
    "        GCNEncoder(dataset.num_features, hidden_channels, out_channels),\n",
    "        VariationalMLPDecoder(out_channels, out_channels, latent_distrib=latent_distrib)\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the trained model state\n",
    "    pre_str = 'edge_' if latent_distrib in ['ks', 'beta', 'tanh-normal'] else ''\n",
    "    model_path = f\"{model_dir}\" + pre_str + f\"{latent_distrib}_{dataset_name}_{run}.pt\"\n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare test edges and features\n",
    "    x = data.x.to(device)\n",
    "    pos_edge_index = data.test_pos_edge_index.to(device)\n",
    "    neg_edge_index = data.test_neg_edge_index.to(device)\n",
    "\n",
    "    metrics = test_posterior_predictive(model, x, pos_edge_index, neg_edge_index)\n",
    "    return metrics\n",
    "#print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "\tBrier Score across Runs: 0.11504 +/- 0.04522\n",
      "Dataset: Citeseer\n",
      "\tBrier Score across Runs: 0.10954 +/- 0.00428\n",
      "Dataset: Pubmed\n",
      "\tBrier Score across Runs: 0.12070 +/- 0.04181\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in [\"Cora\", \"Citeseer\", \"Pubmed\"]:\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    for run in [0, 1, 2, 3, 4]:\n",
    "        metrics = compute_test_metrics_base(dataset_name, run, 5*run)\n",
    "        brier_scores[dataset_name]['base'].append(metrics['brier_score'].item())\n",
    "\n",
    "    print(f\"\\tBrier Score across Runs: {np.mean(brier_scores[dataset_name]['base']):.5f} +/- {np.std(brier_scores[dataset_name]['base']):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'Cora': defaultdict(<class 'list'>, {'ks': [0.08901433646678925, 0.07873287051916122, 0.09745624661445618, 0.06736031174659729, 0.07789163291454315], 'beta': [0.10884460061788559, 0.09924519807100296, 0.13080644607543945, 0.10886358469724655, 0.1347469985485077], 'tanh-normal': [0.08107948303222656, 0.17799915373325348, 0.07496626675128937, 0.08115749061107635, 0.16086654365062714], 'base': [0.08150433003902435, 0.1790006160736084, 0.07329749315977097, 0.08068837970495224, 0.16070735454559326]}), 'Citeseer': defaultdict(<class 'list'>, {'ks': [0.10626861453056335, 0.11789742857217789, 0.10029914230108261, 0.10188400000333786, 0.09374602884054184], 'beta': [0.11231989413499832, 0.16957508027553558, 0.13816359639167786, 0.1460866928100586, 0.13496580719947815], 'tanh-normal': [0.11061416566371918, 0.11421610414981842, 0.11055921763181686, 0.11478753387928009, 0.10277044028043747], 'base': [0.11054332554340363, 0.11482906341552734, 0.11001341789960861, 0.11062514781951904, 0.10171306878328323]}), 'Pubmed': defaultdict(<class 'list'>, {'ks': [0.06285273283720016, 0.05707894638180733, 0.06074433773756027, 0.05549626424908638, 0.06759877502918243], 'beta': [0.06824583560228348, 0.06942050158977509, 0.07306458055973053, 0.06535298377275467, 0.06973250955343246], 'tanh-normal': [0.12236674129962921, 0.19141246378421783, 0.12964662909507751, 0.07344915717840195, 0.08398077636957169], 'base': [0.12348588556051254, 0.1922546774148941, 0.12979918718338013, 0.07388374954462051, 0.08409684896469116]})})\n",
      "dict_keys(['Cora', 'Citeseer', 'Pubmed'])\n",
      "dict_values([defaultdict(<class 'list'>, {'ks': [0.08901433646678925, 0.07873287051916122, 0.09745624661445618, 0.06736031174659729, 0.07789163291454315], 'beta': [0.10884460061788559, 0.09924519807100296, 0.13080644607543945, 0.10886358469724655, 0.1347469985485077], 'tanh-normal': [0.08107948303222656, 0.17799915373325348, 0.07496626675128937, 0.08115749061107635, 0.16086654365062714], 'base': [0.08150433003902435, 0.1790006160736084, 0.07329749315977097, 0.08068837970495224, 0.16070735454559326]}), defaultdict(<class 'list'>, {'ks': [0.10626861453056335, 0.11789742857217789, 0.10029914230108261, 0.10188400000333786, 0.09374602884054184], 'beta': [0.11231989413499832, 0.16957508027553558, 0.13816359639167786, 0.1460866928100586, 0.13496580719947815], 'tanh-normal': [0.11061416566371918, 0.11421610414981842, 0.11055921763181686, 0.11478753387928009, 0.10277044028043747], 'base': [0.11054332554340363, 0.11482906341552734, 0.11001341789960861, 0.11062514781951904, 0.10171306878328323]}), defaultdict(<class 'list'>, {'ks': [0.06285273283720016, 0.05707894638180733, 0.06074433773756027, 0.05549626424908638, 0.06759877502918243], 'beta': [0.06824583560228348, 0.06942050158977509, 0.07306458055973053, 0.06535298377275467, 0.06973250955343246], 'tanh-normal': [0.12236674129962921, 0.19141246378421783, 0.12964662909507751, 0.07344915717840195, 0.08398077636957169], 'base': [0.12348588556051254, 0.1922546774148941, 0.12979918718338013, 0.07388374954462051, 0.08409684896469116]})])\n",
      "Dataset Cora\n",
      "\tmodel: ks - [0.08901433646678925, 0.07873287051916122, 0.09745624661445618, 0.06736031174659729, 0.07789163291454315]\n",
      "\tmodel: beta - [0.10884460061788559, 0.09924519807100296, 0.13080644607543945, 0.10886358469724655, 0.1347469985485077]\n",
      "\tmodel: tanh-normal - [0.08107948303222656, 0.17799915373325348, 0.07496626675128937, 0.08115749061107635, 0.16086654365062714]\n",
      "\tmodel: base - [0.08150433003902435, 0.1790006160736084, 0.07329749315977097, 0.08068837970495224, 0.16070735454559326]\n",
      "Dataset Citeseer\n",
      "\tmodel: ks - [0.10626861453056335, 0.11789742857217789, 0.10029914230108261, 0.10188400000333786, 0.09374602884054184]\n",
      "\tmodel: beta - [0.11231989413499832, 0.16957508027553558, 0.13816359639167786, 0.1460866928100586, 0.13496580719947815]\n",
      "\tmodel: tanh-normal - [0.11061416566371918, 0.11421610414981842, 0.11055921763181686, 0.11478753387928009, 0.10277044028043747]\n",
      "\tmodel: base - [0.11054332554340363, 0.11482906341552734, 0.11001341789960861, 0.11062514781951904, 0.10171306878328323]\n",
      "Dataset Pubmed\n",
      "\tmodel: ks - [0.06285273283720016, 0.05707894638180733, 0.06074433773756027, 0.05549626424908638, 0.06759877502918243]\n",
      "\tmodel: beta - [0.06824583560228348, 0.06942050158977509, 0.07306458055973053, 0.06535298377275467, 0.06973250955343246]\n",
      "\tmodel: tanh-normal - [0.12236674129962921, 0.19141246378421783, 0.12964662909507751, 0.07344915717840195, 0.08398077636957169]\n",
      "\tmodel: base - [0.12348588556051254, 0.1922546774148941, 0.12979918718338013, 0.07388374954462051, 0.08409684896469116]\n"
     ]
    }
   ],
   "source": [
    "print(brier_scores)\n",
    "print(brier_scores.keys())\n",
    "print(brier_scores.values())\n",
    "for ds in brier_scores:\n",
    "    print(f\"Dataset {ds}\")\n",
    "    for m in brier_scores[ds]:\n",
    "        print(f\"\\tmodel: {m} - {brier_scores[ds][m]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maxw/projects/pathwise_grad_kumar/experiments/gnn/logs/calibration_metrics.pkl'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir + 'calibration_metrics.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'Cora': defaultdict(list,\n",
       "                         {'ks': [0.08901433646678925,\n",
       "                           0.07873287051916122,\n",
       "                           0.09745624661445618,\n",
       "                           0.06736031174659729,\n",
       "                           0.07789163291454315],\n",
       "                          'beta': [0.10884460061788559,\n",
       "                           0.09924519807100296,\n",
       "                           0.13080644607543945,\n",
       "                           0.10886358469724655,\n",
       "                           0.1347469985485077],\n",
       "                          'tanh-normal': [0.08107948303222656,\n",
       "                           0.17799915373325348,\n",
       "                           0.07496626675128937,\n",
       "                           0.08115749061107635,\n",
       "                           0.16086654365062714],\n",
       "                          'base': [0.08150433003902435,\n",
       "                           0.1790006160736084,\n",
       "                           0.07329749315977097,\n",
       "                           0.08068837970495224,\n",
       "                           0.16070735454559326]}),\n",
       "             'Citeseer': defaultdict(list,\n",
       "                         {'ks': [0.10626861453056335,\n",
       "                           0.11789742857217789,\n",
       "                           0.10029914230108261,\n",
       "                           0.10188400000333786,\n",
       "                           0.09374602884054184],\n",
       "                          'beta': [0.11231989413499832,\n",
       "                           0.16957508027553558,\n",
       "                           0.13816359639167786,\n",
       "                           0.1460866928100586,\n",
       "                           0.13496580719947815],\n",
       "                          'tanh-normal': [0.11061416566371918,\n",
       "                           0.11421610414981842,\n",
       "                           0.11055921763181686,\n",
       "                           0.11478753387928009,\n",
       "                           0.10277044028043747],\n",
       "                          'base': [0.11054332554340363,\n",
       "                           0.11482906341552734,\n",
       "                           0.11001341789960861,\n",
       "                           0.11062514781951904,\n",
       "                           0.10171306878328323]}),\n",
       "             'Pubmed': defaultdict(list,\n",
       "                         {'ks': [0.06285273283720016,\n",
       "                           0.05707894638180733,\n",
       "                           0.06074433773756027,\n",
       "                           0.05549626424908638,\n",
       "                           0.06759877502918243],\n",
       "                          'beta': [0.06824583560228348,\n",
       "                           0.06942050158977509,\n",
       "                           0.07306458055973053,\n",
       "                           0.06535298377275467,\n",
       "                           0.06973250955343246],\n",
       "                          'tanh-normal': [0.12236674129962921,\n",
       "                           0.19141246378421783,\n",
       "                           0.12964662909507751,\n",
       "                           0.07344915717840195,\n",
       "                           0.08398077636957169],\n",
       "                          'base': [0.12348588556051254,\n",
       "                           0.1922546774148941,\n",
       "                           0.12979918718338013,\n",
       "                           0.07388374954462051,\n",
       "                           0.08409684896469116]})})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maxw/projects/pathwise_grad_kumar/experiments/gnn/logs/calibration_metrics.pkl'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir + 'calibration_metrics.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the calibration metrics to a file\n",
    "import pickle\n",
    "with open(log_dir + 'calibration_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(brier_scores, f)\n",
    "    f.flush()  # Ensure data is written to disk\n",
    "    os.fsync(f.fileno())  # Force write to disk (optional)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kumar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
